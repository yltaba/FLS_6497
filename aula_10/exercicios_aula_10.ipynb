{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios 10\n",
    "## Aula 10\n",
    "\n",
    "## 1) Tuning\n",
    "Neste exercício usaremos uma base de dados com decisões da Suprema Corte americana que contém informações sobre os casos julgados e posições dos juízes em suas arguições, entre outros (Kaufman, Kraft, e Sen 2019). No paper1, o resultado de acurácia encontrado é de 74% com um AdaBoost. Seu desafio é tentar replicar, e potencialmente superar, esse resultado – ou, melhor, ver se é possível replicar e superar o mesmo resultado montando uma pipeline do zero. Detalhes importantes:\n",
    "\n",
    "- Nosso target é a variável winner, que indica se uma dada petição foi vitoriosa no plenário\n",
    "\n",
    "- Teste outras métricas de validação (note que há o dobro de decisões positivas na base)\n",
    "\n",
    "- Pense na melhor estratégia de validação a usar (o estudo original usa 10-fold cross validation) e justifique sua escolha (em comentários no código)\n",
    "\n",
    "- Analise as variáveis na base e veja se não é possível pré-processar (ou mesmo remover) algumas que talvez sejam problemáticas\n",
    "\n",
    "- Teste diferentes pipelines, com diferentes modelos e hiper-parâmetros\n",
    "\n",
    "Os dados já limpos estão no GitHub do curso e podem ser carregados com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://github.com/FLS-6497/datasets/raw/main/aula10/supreme.csv'\n",
    "dados = pd.read_csv(link, sep=';', decimal=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Tuning com text as data\n",
    "Neste exercício revisitaremos os dados do Projeto 1 para aplicar tuning às pipelines que vocês já montaram anteriormente (é possível ir no GitHub consultar seu código). Particularmente, tuning será útil para identificar melhores combinações de hiper-parâmetros de pré-processamento – número ou proporção mínima de ocorrência de palavras, número mínimo de ocorrência de uma palavra entre documentos, tamanho do N em N - grams, etc.\n",
    "\n",
    "Carregue os dados com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://github.com/FLS-6497/datasets/blob/main/projeto1/discursos_pres_internacionais.csv?raw=true'\n",
    "discursos = pd.read_csv(link, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Melhorando as predições climáticas\n",
    "Neste exercício final, usaremos tuning para dar um passo adicional na tarefa de predizer a temparatura máxima diária em São Bernardo do Campo (SP). Para isso, use seu código da última aula e o adapte para fazer tuning de hiper-parâmetros (é possível usar o dicionário do mlr3 já com combinações prontas de hiper-parâmetros).\n",
    "\n",
    "Carregue os dados com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv'\n",
    "dados = pd.read_csv(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao final, valide a sua melhor pipeline com dados de Campinas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Campinas_Cleaned.csv'\n",
    "campinas = pd.read_csv(campinas)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
